{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(text): #### Cleaning Tweets\n",
    " re2 = re.sub(\"[^A-Za-z]+\",\" \", text) # removing numbers\n",
    " tokens = nltk.word_tokenize(re2)\n",
    " removed_letters = [word for word in tokens if len(word)>2] # removing words\n",
    " lower_case = [l.lower() for l in removed_letters]\n",
    " stop_words = set(stopwords.words('english'))\n",
    " filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    " wordnet_lemmatizer = WordNetLemmatizer()\n",
    " lemmas = ' '.join([wordnet_lemmatizer.lemmatize(t, pos='v') for t in filtered_result])\n",
    " return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(train_text,val_text,ngram,thresh):\n",
    "    #feature extraction using tf-0idf\n",
    "    vectorizer=TfidfVectorizer(analyzer='word',max_features=1500,ngram_range=(ngram),max_df=1.0,min_df=thresh)  \n",
    "    train_vector=vectorizer.fit_transform(train_text) \n",
    "    val_vector=vectorizer.transform(val_text)\n",
    "    return train_vector,val_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(labels):\n",
    "    #convert labels to binary \n",
    "    labels[labels<8]=0\n",
    "    labels[labels>=8]=1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(df,model,ngram,thresh):\n",
    "    #apply k fold cross validation\n",
    "    accuracy,f1,precision,recall=[],[],[],[]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)  #k fold cross validatoon\n",
    "    for train_idx,val_idx in kf.split(df):\n",
    "        train=df.iloc[train_idx,:] #slice training data\n",
    "        val=df.iloc[val_idx,:] #slice validation data\n",
    "        \n",
    "        train_text,train_labels=train['extract'].copy(),label_encode(train['score'].copy()) #slice features and labels seprate for training data\n",
    "        val_text,val_labels=val['extract'].copy(),label_encode(val['score'].copy())#slice features and labels seprate for validation data\n",
    "        train_vector,val_vector=extract_features(train_text,val_text,ngram,thresh)#feature extraction\n",
    "        #fit the model\n",
    "        model.fit(train_vector,train_labels)\n",
    "        y_pred=model.predict(val_vector)\n",
    "        y_pred=y_pred.astype('int') #convert float to integers\n",
    "        accuracy.append(accuracy_score(val_labels,y_pred)) #accuracy\n",
    "        f1.append(f1_score(val_labels,y_pred,average='macro')) #f1 score\n",
    "        precision.append(precision_score(val_labels,y_pred,average='macro')) #precison\n",
    "        recall.append(recall_score(val_labels,y_pred,average='macro')) #recall\n",
    "    \n",
    "    #save model resuls in a dataframe\n",
    "    result=pd.DataFrame(data=list(zip(accuracy,precision,recall,f1)),columns=['Accuracy','Precision','Recall','F1-score'])\n",
    "    result.loc['Average'] = result.mean() #calculate average of 5 fold cross validation\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(df,model,ngram,thresh):\n",
    "    #save the model\n",
    "    labels=df['score']\n",
    "    labels[labels<8]=0 \n",
    "    labels[labels>=8]=1\n",
    "    #apply tf idf\n",
    "    vectorizer=TfidfVectorizer(analyzer='word',max_features=1500,ngram_range=(ngram),max_df=1.0,min_df=thresh)  \n",
    "    data=vectorizer.fit_transform(df['extract'])\n",
    "    #save model\n",
    "    joblib.dump(vectorizer,'vec.pkl')\n",
    "    model.fit(data,labels)\n",
    "    joblib.dump(model,'clf.pkl')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df=pd.read_csv('user_reviews.csv') #read file\n",
    "    df['extract']=df['extract'].apply(normalizer) #apply preporcessing\n",
    "    model1=MultinomialNB() #naive bayes moels\n",
    "    res1=cross_validation(df,model1,ngram=(1,1),thresh=10) # naive bayes unigram\n",
    "    res2=cross_validation(df,model1,ngram=(1,2),thresh=20) # naive bayes bigram \n",
    "    res3=cross_validation(df,model1,ngram=(1,3),thresh=30) # naive bayes trigram\n",
    "    \n",
    "    #run svm model\n",
    "    model2=LinearSVC()\n",
    "    res12=cross_validation(df,model2,ngram=(1,1),thresh=10) #svm unigram\n",
    "    res22=cross_validation(df,model2,ngram=(1,2),thresh=20) #svm bigram\n",
    "    res32=cross_validation(df,model2,ngram=(1,3),thresh=30) #svm trigram\n",
    "    \n",
    "    #run adda boost model\n",
    "    model3=AdaBoostClassifier()\n",
    "    res13=cross_validation(df,model3,ngram=(1,1),thresh=10) #adaboost unigram\n",
    "    res23=cross_validation(df,model3,ngram=(1,2),thresh=20) #adaboost bigram\n",
    "    res33=cross_validation(df,model3,ngram=(1,3),thresh=30) # #adaboost trigram\n",
    "    \n",
    "    #extract average f1 score of all models\n",
    "    out1f=res1.loc['Average']['F1-score']\n",
    "    out2f=res2.loc['Average']['F1-score']\n",
    "    out3f=res3.loc['Average']['F1-score']\n",
    "    \n",
    "    out4f=res12.loc['Average']['F1-score']\n",
    "    out5f=res22.loc['Average']['F1-score']\n",
    "    out6f=res32.loc['Average']['F1-score']\n",
    "    \n",
    "    out7f=res13.loc['Average']['F1-score']\n",
    "    out8f=res23.loc['Average']['F1-score']\n",
    "    out9f=res33.loc['Average']['F1-score']\n",
    "    \n",
    "    #create a list containing score of all models\n",
    "    lis=[out1f,out2f,out3f,out4f,out5f,out6f,out7f,out8f,out9f]\n",
    "    \n",
    "    #save the model which has the best score\n",
    "    if np.argmax(lis)==0:\n",
    "        save_model(df,model1,(1,1),10)\n",
    "        \n",
    "    elif np.argmax(lis)==1:\n",
    "        save_model(df,model1,(1,2),20)\n",
    "   \n",
    "    elif np.argmax(lis)==2:\n",
    "        save_model(df,model1,(1,3),30)\n",
    "        \n",
    "    elif np.argmax(lis)==3:\n",
    "        save_model(df,model2,(1,1),10)\n",
    "        \n",
    "    elif np.argmax(lis)==4:\n",
    "        save_model(df,model2,(1,2),20)\n",
    "        \n",
    "    elif np.argmax(lis)==5:\n",
    "       save_model(df,model2,(1,1),30)\n",
    "  \n",
    "    elif np.argmax(lis)==6:\n",
    "        save_model(df,model3,(1,1),10)\n",
    " \n",
    "    elif np.argmax(lis)==7:\n",
    "        save_model(df,model3,(1,2),20)\n",
    "\n",
    "    elif np.argmax(lis)==8:\n",
    "        save_model(df,model3,(1,3),30)\n",
    "\n",
    "    #add all results to dataframe\n",
    "    #result=pd.concat([res1,res2,res3,res12,res22,res32,res13,res23,res33],axis=1)\n",
    "    \n",
    "    return res1,res2,res3,res12,res22,res32,res13,res23,res33\n",
    "\n",
    "\n",
    "res1,res2,res3,res12,res22,res32,res13,res23,res33=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Naive Bayes\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.84755   0.842577  0.726429  0.759234\n",
      "1         0.84995   0.843868  0.730195  0.763070\n",
      "2         0.84895   0.845446  0.727272  0.760574\n",
      "3         0.85075   0.849483  0.729948  0.763700\n",
      "4         0.85065   0.844468  0.731306  0.764220\n",
      "Average   0.84957   0.845168  0.729030  0.762160\n"
     ]
    }
   ],
   "source": [
    "print('Unigram Naive Bayes')\n",
    "print(res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram Naive Bayes\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.85230   0.838907  0.742696  0.773103\n",
      "1         0.85655   0.844024  0.748825  0.779532\n",
      "2         0.85600   0.843369  0.748693  0.779227\n",
      "3         0.85435   0.843012  0.744741  0.775743\n",
      "4         0.85410   0.839456  0.745164  0.775460\n",
      "Average   0.85466   0.841754  0.746024  0.776613\n"
     ]
    }
   ],
   "source": [
    "print('bigram Naive Bayes')\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram Naive Bayes\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.85185   0.837485  0.742597  0.772721\n",
      "1         0.85625   0.842837  0.748963  0.779391\n",
      "2         0.85560   0.841850  0.748830  0.779013\n",
      "3         0.85480   0.842283  0.746649  0.777226\n",
      "4         0.85445   0.838989  0.746614  0.776588\n",
      "Average   0.85459   0.840689  0.746731  0.776988\n"
     ]
    }
   ],
   "source": [
    "print('trigram Naive Bayes')\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram SYM\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.87085   0.841994  0.798171  0.816324\n",
      "1         0.87125   0.839412  0.801595  0.817682\n",
      "2         0.87160   0.841821  0.800124  0.817574\n",
      "3         0.87395   0.845923  0.803020  0.820926\n",
      "4         0.87050   0.839208  0.798741  0.815754\n",
      "Average   0.87163   0.841672  0.800330  0.817652\n"
     ]
    }
   ],
   "source": [
    "print('Unigram SYM')\n",
    "print(res12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram SYM\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.86955   0.839458  0.797238  0.814819\n",
      "1         0.87220   0.841392  0.802024  0.818678\n",
      "2         0.87365   0.844138  0.804041  0.820970\n",
      "3         0.87280   0.843500  0.802455  0.819696\n",
      "4         0.87100   0.838455  0.801778  0.817454\n",
      "Average   0.87184   0.841389  0.801507  0.818323\n"
     ]
    }
   ],
   "source": [
    "print('bigram SYM')\n",
    "print(res22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram SYM\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.86950   0.839449  0.797071  0.814705\n",
      "1         0.87130   0.840471  0.800143  0.817119\n",
      "2         0.87390   0.844585  0.804275  0.821283\n",
      "3         0.87240   0.842938  0.801854  0.819102\n",
      "4         0.87075   0.838142  0.801341  0.817058\n",
      "Average   0.87157   0.841117  0.800937  0.817853\n"
     ]
    }
   ],
   "source": [
    "print('trigram SYM')\n",
    "print(res32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Adaboost\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.81910   0.784969  0.688623  0.714439\n",
      "1         0.82255   0.788960  0.693547  0.719859\n",
      "2         0.82300   0.788528  0.696966  0.722918\n",
      "3         0.82595   0.796644  0.699227  0.726323\n",
      "4         0.81940   0.778662  0.693558  0.718182\n",
      "Average   0.82200   0.787553  0.694384  0.720344\n"
     ]
    }
   ],
   "source": [
    "print('Unigram Adaboost')\n",
    "print(res13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram Adaboost\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.81870   0.783069  0.689292  0.714788\n",
      "1         0.83000   0.791493  0.717603  0.741737\n",
      "2         0.82545   0.791772  0.701956  0.728055\n",
      "3         0.82570   0.796581  0.698458  0.725591\n",
      "4         0.82250   0.785773  0.696362  0.721978\n",
      "Average   0.82447   0.789738  0.700734  0.726430\n"
     ]
    }
   ],
   "source": [
    "print('bigram Adaboost')\n",
    "print(res23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram Adaboost\n",
      "         Accuracy  Precision    Recall  F1-score\n",
      "0         0.82055   0.785579  0.693065  0.718738\n",
      "1         0.82500   0.790761  0.700171  0.726306\n",
      "2         0.82425   0.791563  0.697999  0.724381\n",
      "3         0.82800   0.798480  0.704211  0.731250\n",
      "4         0.82270   0.783784  0.699538  0.724514\n",
      "Average   0.82410   0.790033  0.698997  0.725038\n"
     ]
    }
   ],
   "source": [
    "print('trigram Adaboost')\n",
    "print(res33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
